{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c31423",
   "metadata": {},
   "source": [
    "## 이미지 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b1c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벚꽃 : 442\n",
      "놀이공원 : 348\n",
      "타워 : 318\n",
      "유람선 : 366\n",
      "케이블카 : 307\n",
      "빌딩 : 454\n",
      "노을 : 393\n",
      "번화가 : 335\n",
      "돌담길 : 704\n",
      "궁 : 363\n",
      "철로 : 434\n",
      "자연 : 705\n",
      "아쿠아리움 : 334\n",
      "피크닉 : 410\n",
      "아이스링크 : 82\n",
      "단풍 : 341\n",
      "전통문화 : 301\n",
      "야경 : 431\n",
      "도심하천 : 791\n",
      "극장 : 865\n",
      "워터파크 : 320\n",
      "['/Users/nayoungmin/Desktop/Capstone/image_yeju/벚꽃', '/Users/nayoungmin/Desktop/Capstone/image_yeju/놀이공원', '/Users/nayoungmin/Desktop/Capstone/image_yeju/타워', '/Users/nayoungmin/Desktop/Capstone/image_yeju/유람선', '/Users/nayoungmin/Desktop/Capstone/image_yeju/케이블카', '/Users/nayoungmin/Desktop/Capstone/image_yeju/빌딩', '/Users/nayoungmin/Desktop/Capstone/image_yeju/노을', '/Users/nayoungmin/Desktop/Capstone/image_yeju/번화가', '/Users/nayoungmin/Desktop/Capstone/image_yeju/돌담길', '/Users/nayoungmin/Desktop/Capstone/image_yeju/궁', '/Users/nayoungmin/Desktop/Capstone/image_yeju/철로', '/Users/nayoungmin/Desktop/Capstone/image_yeju/자연', '/Users/nayoungmin/Desktop/Capstone/image_yeju/아쿠아리움', '/Users/nayoungmin/Desktop/Capstone/image_yeju/피크닉', '/Users/nayoungmin/Desktop/Capstone/image_yeju/아이스링크', '/Users/nayoungmin/Desktop/Capstone/image_yeju/단풍', '/Users/nayoungmin/Desktop/Capstone/image_yeju/전통문화', '/Users/nayoungmin/Desktop/Capstone/image_yeju/야경', '/Users/nayoungmin/Desktop/Capstone/image_yeju/도심하천', '/Users/nayoungmin/Desktop/Capstone/image_yeju/극장', '/Users/nayoungmin/Desktop/Capstone/image_yeju/워터파크']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"/Users/nayoungmin/Desktop/Capstone/image_yeju\"\n",
    "file_names = [f for f in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, f))]\n",
    "\n",
    "file_Path = []\n",
    "# 장소별 데이터 수\n",
    "for place in file_names:\n",
    "    PATH = os.path.join(file_path, place)\n",
    "    print(str(place), \":\", str(len(os.listdir(PATH))))\n",
    "    file_Path.append(PATH)\n",
    "\n",
    "print(file_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246ba76",
   "metadata": {},
   "source": [
    "## GPU 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70be8e",
   "metadata": {},
   "source": [
    "## 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c42caa",
   "metadata": {},
   "source": [
    "### GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef22a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except:\n",
    "        print(\"Cannot set memory growth on device\")\n",
    "    else:\n",
    "        print(\"GPU memory growth set to True\")\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89b5d6",
   "metadata": {},
   "source": [
    "## train, validation 나누기\n",
    "7.5:2.5 비율로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66f4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders   # 입력된 비율을 기준으로 데이터셋을 분할하는 함수\n",
    "input_folder = \"/Users/nayoungmin/Desktop/Capstone/image_yeju\"\n",
    "output = \"/Users/nayoungmin/Desktop/Capstone/splited_image_is\" # where you want the split datasets saved. one will be created if it does not exist or none is set\n",
    "\n",
    "splitfolders.ratio(input_folder, output=output, seed=777, ratio=(0.75, 0.25)) # 7.5:2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4e7a5",
   "metadata": {},
   "source": [
    "## class별 데이터 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956dccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ train data ]\n",
      "\n",
      "벚꽃  :  330\n",
      "놀이공원  :  261\n",
      "타워  :  237\n",
      "유람선  :  274\n",
      "케이블카  :  230\n",
      "빌딩  :  339\n",
      "노을  :  294\n",
      "번화가  :  251\n",
      "트리  :  258\n",
      "돌담길  :  527\n",
      "호수공원  :  306\n",
      "궁  :  272\n",
      "철로  :  324\n",
      "자연  :  528\n",
      "아쿠아리움  :  250\n",
      "아이스링크  :  61\n",
      "단풍  :  255\n",
      "전통문화  :  225\n",
      "야경  :  323\n",
      "도심하천  :  592\n",
      "극장  :  648\n",
      "워터파크  :  240\n",
      "\n",
      "[ val data ]\n",
      "\n",
      "벚꽃  :  111\n",
      "놀이공원  :  87\n",
      "타워  :  80\n",
      "유람선  :  92\n",
      "케이블카  :  77\n",
      "빌딩  :  114\n",
      "노을  :  99\n",
      "번화가  :  84\n",
      "트리  :  87\n",
      "돌담길  :  176\n",
      "호수공원  :  103\n",
      "궁  :  91\n",
      "철로  :  109\n",
      "자연  :  176\n",
      "아쿠아리움  :  84\n",
      "아이스링크  :  21\n",
      "단풍  :  86\n",
      "전통문화  :  76\n",
      "야경  :  108\n",
      "도심하천  :  198\n",
      "극장  :  216\n",
      "워터파크  :  80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '/Users/nayoungmin/Desktop/Capstone/splited_image_is'\n",
    "file_names = [f for f in os.listdir(file_path) if not f.startswith('.')]  # 숨김파일 제외\n",
    "\n",
    "for data in file_names:\n",
    "    PATH = os.path.join(file_path,data)\n",
    "    plan_file = [f for f in os.listdir(PATH) if not f.startswith('.')]  # 숨김파일 제외\n",
    "    print(\"[\",str(data),\"data ]\")\n",
    "    print()\n",
    "    for plan in plan_file:\n",
    "        PATH_ = os.path.join(PATH,plan)\n",
    "        print(str(plan),\" : \",str(len(os.listdir(PATH_))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bd405",
   "metadata": {},
   "source": [
    "## 이미지 전처리 (이미지 증강 기법 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97795c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7025 images belonging to 22 classes.\n",
      "Found 2355 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAIN_PATH = '/Users/nayoungmin/Desktop/Capstone/splited_image_is/train'\n",
    "VAL_PATH = '/Users/nayoungmin/Desktop/Capstone/splited_image_is/val'\n",
    "\n",
    "\n",
    "MODEL_PATH = '/Users/nayoungmin/Desktop/Capstone/model'\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "IMG_HEIGHT = 224  \n",
    "IMG_WIDTH = 224 \n",
    "N_EPOCH = 15\n",
    "LR = 0.05\n",
    "nb_classes = 82\n",
    "\n",
    "trainGen = ImageDataGenerator(\n",
    "    rescale=1./255,        # 값을 0과 1 사이로 변경\n",
    "    rotation_range=30,      # 무작위 회전각도 30도 이내\n",
    "    shear_range=0.2,         # 층밀리기 강도 20% (정사각형 -> 평행사변형)\n",
    "    zoom_range=0.2,            # 무작위 줌 범위 20%\n",
    "    horizontal_flip=True    # 무작위로 가로로 뒤짚는다.\n",
    ")\n",
    "valGen = ImageDataGenerator(\n",
    "    rescale=1./255,       \n",
    "    rotation_range=30,   \n",
    "    shear_range=0.2,      \n",
    "    zoom_range=0.2,            \n",
    "    horizontal_flip=True    \n",
    ")\n",
    "\n",
    "train_generator = trainGen.flow_from_directory(      # flow_from_directory() : 각 이미지마다 디렉토리 이름을 레이블로 사용\n",
    "   TRAIN_PATH,\n",
    "   class_mode=\"categorical\",\n",
    "   target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "   shuffle=True,\n",
    "   batch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the validation generator\n",
    "validation_generator = valGen.flow_from_directory(  # flow_from_directory() : 각 이미지마다 디렉토리 이름을 레이블로 사용\n",
    "   VAL_PATH,\n",
    "   class_mode=\"categorical\",\n",
    "   target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "   shuffle=True,\n",
    "   batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aefad6",
   "metadata": {},
   "source": [
    "## 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca9f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/f448gzfd2rd9vk0q5h6wlvn80000gn/T/ipykernel_2779/4159987933.py:56: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  H = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 14:38:00.761327: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 63s 436ms/step - loss: 0.8442 - accuracy: 0.7508 - AUC: 0.9710 - val_loss: 0.5340 - val_accuracy: 0.8485 - val_AUC: 0.9862 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/14\n",
      "140/140 [==============================] - 57s 406ms/step - loss: 0.4526 - accuracy: 0.8594 - AUC: 0.9907 - val_loss: 0.3942 - val_accuracy: 0.8715 - val_AUC: 0.9914 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/14\n",
      "140/140 [==============================] - 58s 414ms/step - loss: 0.3650 - accuracy: 0.8834 - AUC: 0.9936 - val_loss: 0.4080 - val_accuracy: 0.8838 - val_AUC: 0.9891 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/14\n",
      "140/140 [==============================] - 57s 406ms/step - loss: 0.3162 - accuracy: 0.9009 - AUC: 0.9949 - val_loss: 0.4014 - val_accuracy: 0.8774 - val_AUC: 0.9896 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/14\n",
      "140/140 [==============================] - 56s 401ms/step - loss: 0.2837 - accuracy: 0.9115 - AUC: 0.9958 - val_loss: 0.3704 - val_accuracy: 0.8923 - val_AUC: 0.9898 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/14\n",
      "140/140 [==============================] - 58s 411ms/step - loss: 0.2660 - accuracy: 0.9108 - AUC: 0.9964 - val_loss: 0.3777 - val_accuracy: 0.8868 - val_AUC: 0.9907 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/14\n",
      "140/140 [==============================] - 56s 398ms/step - loss: 0.2459 - accuracy: 0.9223 - AUC: 0.9967 - val_loss: 0.3474 - val_accuracy: 0.8966 - val_AUC: 0.9909 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/14\n",
      "140/140 [==============================] - 56s 396ms/step - loss: 0.2278 - accuracy: 0.9260 - AUC: 0.9972 - val_loss: 0.3698 - val_accuracy: 0.8902 - val_AUC: 0.9901 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/14\n",
      "140/140 [==============================] - 56s 399ms/step - loss: 0.2144 - accuracy: 0.9326 - AUC: 0.9969 - val_loss: 0.3516 - val_accuracy: 0.9004 - val_AUC: 0.9902 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/14\n",
      "140/140 [==============================] - 55s 395ms/step - loss: 0.2110 - accuracy: 0.9290 - AUC: 0.9969 - val_loss: 0.3203 - val_accuracy: 0.9068 - val_AUC: 0.9924 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 11/14\n",
      "140/140 [==============================] - 55s 394ms/step - loss: 0.1769 - accuracy: 0.9405 - AUC: 0.9980 - val_loss: 0.2990 - val_accuracy: 0.9119 - val_AUC: 0.9926 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 12/14\n",
      "140/140 [==============================] - 55s 392ms/step - loss: 0.1555 - accuracy: 0.9521 - AUC: 0.9981 - val_loss: 0.2963 - val_accuracy: 0.9085 - val_AUC: 0.9938 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 13/14\n",
      "140/140 [==============================] - 55s 391ms/step - loss: 0.1394 - accuracy: 0.9563 - AUC: 0.9987 - val_loss: 0.3125 - val_accuracy: 0.9123 - val_AUC: 0.9911 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 14/14\n",
      "140/140 [==============================] - 55s 392ms/step - loss: 0.1342 - accuracy: 0.9564 - AUC: 0.9990 - val_loss: 0.2857 - val_accuracy: 0.9170 - val_AUC: 0.9930 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers  import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.metrics import AUC\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "# ResNet50V2 모델을 불러오고, 그 모델의 출력을 활용하여 새로운 fully connected layer를 추가\n",
    "resnet = ResNet50V2(include_top=False, weights='imagenet', input_shape = (224,224,3))\n",
    "\n",
    "x = resnet.output\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x =  Dense(512, activation='relu', input_dim= (224,224,3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(22, activation='softmax')(x)  # 여기에서 Dense를 class 수에 맞게 설정해야 함.\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "\n",
    "# 기존 학습된 layer들을 freazing(새로운 layer들만 학습하도록)\n",
    "for layer in resnet.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "# define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        lr = lr * 0.1\n",
    "    return lr\n",
    "\n",
    "# compile the model with the learning rate scheduler\n",
    "opt = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\",AUC(multi_label=True,num_labels=22,name='AUC')])\n",
    "\n",
    "# train the model\n",
    "H = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=14,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
    ")\n",
    "\n",
    "model.save('my_model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ac589",
   "metadata": {},
   "source": [
    "## 예측\n",
    "이미지 넣어서 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c7b9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "궁: 56.98%\n",
      "극장: 0.01%\n",
      "노을: 0.0%\n",
      "놀이공원: 0.02%\n",
      "단풍: 0.02%\n",
      "도심하천: 0.02%\n",
      "돌담길: 0.0%\n",
      "번화가: 0.0%\n",
      "벚꽃: 0.26%\n",
      "빌딩: 0.37%\n",
      "아이스링크: 0.0%\n",
      "아쿠아리움: 0.01%\n",
      "야경: 0.03%\n",
      "워터파크: 0.0%\n",
      "유람선: 0.04%\n",
      "자연: 0.0%\n",
      "전통문화: 0.11%\n",
      "철로: 0.01%\n",
      "케이블카: 0.0%\n",
      "타워: 41.84%\n",
      "트리: 0.28%\n",
      "호수공원: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "img_path = '/Users/nayoungmin/Desktop/스크린샷 2023-04-11 오후 3.46.20.png' # 예측하고자 하는 이미지 경로\n",
    "target_size = (224, 224) # 모델이 학습시킨 이미지 사이즈와 동일하게 설정\n",
    "\n",
    "# 이미지 전처리\n",
    "img = load_img(img_path, target_size=target_size)\n",
    "x = img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x / 255.0 # 이미지 스케일링\n",
    "\n",
    "# 예측\n",
    "preds = model.predict(x)\n",
    "\n",
    "# 클래스별 확률 값 출력\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "  print(f\"{class_names[i]}: {round(preds[0][i]*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccdd3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
