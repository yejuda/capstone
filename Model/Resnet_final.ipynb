{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c31423",
   "metadata": {},
   "source": [
    "### 이미지 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b1c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벚꽃 : 442\n",
      "놀이공원 : 348\n",
      "타워 : 318\n",
      "유람선 : 366\n",
      "케이블카 : 307\n",
      "빌딩 : 454\n",
      "노을 : 393\n",
      "번화가 : 335\n",
      "돌담길 : 704\n",
      "궁 : 363\n",
      "철로 : 434\n",
      "자연 : 705\n",
      "아쿠아리움 : 334\n",
      "피크닉 : 410\n",
      "아이스링크 : 82\n",
      "단풍 : 341\n",
      "전통문화 : 301\n",
      "야경 : 431\n",
      "도심하천 : 791\n",
      "극장 : 865\n",
      "워터파크 : 320\n",
      "['/Users/nayoungmin/Desktop/Capstone/image_yeju/벚꽃', '/Users/nayoungmin/Desktop/Capstone/image_yeju/놀이공원', '/Users/nayoungmin/Desktop/Capstone/image_yeju/타워', '/Users/nayoungmin/Desktop/Capstone/image_yeju/유람선', '/Users/nayoungmin/Desktop/Capstone/image_yeju/케이블카', '/Users/nayoungmin/Desktop/Capstone/image_yeju/빌딩', '/Users/nayoungmin/Desktop/Capstone/image_yeju/노을', '/Users/nayoungmin/Desktop/Capstone/image_yeju/번화가', '/Users/nayoungmin/Desktop/Capstone/image_yeju/돌담길', '/Users/nayoungmin/Desktop/Capstone/image_yeju/궁', '/Users/nayoungmin/Desktop/Capstone/image_yeju/철로', '/Users/nayoungmin/Desktop/Capstone/image_yeju/자연', '/Users/nayoungmin/Desktop/Capstone/image_yeju/아쿠아리움', '/Users/nayoungmin/Desktop/Capstone/image_yeju/피크닉', '/Users/nayoungmin/Desktop/Capstone/image_yeju/아이스링크', '/Users/nayoungmin/Desktop/Capstone/image_yeju/단풍', '/Users/nayoungmin/Desktop/Capstone/image_yeju/전통문화', '/Users/nayoungmin/Desktop/Capstone/image_yeju/야경', '/Users/nayoungmin/Desktop/Capstone/image_yeju/도심하천', '/Users/nayoungmin/Desktop/Capstone/image_yeju/극장', '/Users/nayoungmin/Desktop/Capstone/image_yeju/워터파크']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"/Users/nayoungmin/Desktop/Capstone/image_yeju\"\n",
    "file_names = [f for f in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, f))]\n",
    "\n",
    "file_Path = []\n",
    "# 장소별 데이터 수\n",
    "for place in file_names:\n",
    "    PATH = os.path.join(file_path, place)\n",
    "    print(str(place), \":\", str(len(os.listdir(PATH))))\n",
    "    file_Path.append(PATH)\n",
    "\n",
    "print(file_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a353e57",
   "metadata": {},
   "source": [
    "## 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02aef69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import splitfolders   # 입력된 비율을 기준으로 데이터셋을 분할하는 함수\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers  import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "from keras.metrics import AUC\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246ba76",
   "metadata": {},
   "source": [
    "## GPU 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70be8e",
   "metadata": {},
   "source": [
    "### 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c42caa",
   "metadata": {},
   "source": [
    "### GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef22a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth set to True\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except:\n",
    "        print(\"Cannot set memory growth on device\")\n",
    "    else:\n",
    "        print(\"GPU memory growth set to True\")\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66f4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders   # 입력된 비율을 기준으로 데이터셋을 분할하는 함수\n",
    "input_folder = \"/Users/nayoungmin/Desktop/Capstone/image_yeju\"\n",
    "output = \"/Users/nayoungmin/Desktop/Capstone/splited_image_is\" # where you want the split datasets saved. one will be created if it does not exist or none is set\n",
    "\n",
    "splitfolders.ratio(input_folder, output=output, seed=777, ratio=(0.7, 0.3)) # train,test를 8:2 split 후, train,val을 9:1로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956dccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ train data ]\n",
      "\n",
      "벚꽃  :  308\n",
      "놀이공원  :  243\n",
      "타워  :  221\n",
      "유람선  :  256\n",
      "케이블카  :  214\n",
      "빌딩  :  317\n",
      "노을  :  275\n",
      "번화가  :  234\n",
      "돌담길  :  492\n",
      "궁  :  254\n",
      "철로  :  303\n",
      "자연  :  492\n",
      "아쿠아리움  :  233\n",
      "피크닉  :  286\n",
      "아이스링크  :  57\n",
      "단풍  :  238\n",
      "전통문화  :  210\n",
      "야경  :  301\n",
      "도심하천  :  553\n",
      "극장  :  604\n",
      "워터파크  :  224\n",
      "\n",
      "[ val data ]\n",
      "\n",
      "벚꽃  :  133\n",
      "놀이공원  :  105\n",
      "타워  :  96\n",
      "유람선  :  110\n",
      "케이블카  :  93\n",
      "빌딩  :  136\n",
      "노을  :  118\n",
      "번화가  :  101\n",
      "돌담길  :  211\n",
      "궁  :  109\n",
      "철로  :  130\n",
      "자연  :  212\n",
      "아쿠아리움  :  101\n",
      "피크닉  :  123\n",
      "아이스링크  :  25\n",
      "단풍  :  103\n",
      "전통문화  :  91\n",
      "야경  :  130\n",
      "도심하천  :  237\n",
      "극장  :  260\n",
      "워터파크  :  96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '/Users/nayoungmin/Desktop/Capstone/splited_image_is'\n",
    "file_names = [f for f in os.listdir(file_path) if not f.startswith('.')]  # 숨김파일 제외\n",
    "\n",
    "for data in file_names:\n",
    "    PATH = os.path.join(file_path,data)\n",
    "    plan_file = [f for f in os.listdir(PATH) if not f.startswith('.')]  # 숨김파일 제외\n",
    "    print(\"[\",str(data),\"data ]\")\n",
    "    print()\n",
    "    for plan in plan_file:\n",
    "        PATH_ = os.path.join(PATH,plan)\n",
    "        print(str(plan),\" : \",str(len(os.listdir(PATH_))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97795c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6315 images belonging to 21 classes.\n",
      "Found 2720 images belonging to 21 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAIN_PATH = '/Users/nayoungmin/Desktop/Capstone/splited_image_is/train'\n",
    "VAL_PATH = '/Users/nayoungmin/Desktop/Capstone/splited_image_is/val'\n",
    "\n",
    "\n",
    "MODEL_PATH = '/Users/nayoungmin/Desktop/Capstone/Code'\n",
    "\n",
    "BATCH_SIZE = 50 #배치사이즈 원래 50\n",
    "IMG_HEIGHT = 224  \n",
    "IMG_WIDTH = 224 \n",
    "N_EPOCH = 40\n",
    "LR = 0.05\n",
    "nb_classes = 82\n",
    "\n",
    "trainGen = ImageDataGenerator(\n",
    "    rescale=1./255,        # 값을 0과 1 사이로 변경\n",
    "    rotation_range=30,      # 무작위 회전각도 30도 이내\n",
    "    shear_range=0.2,         # 층밀리기 강도 20% (정사각형 -> 평행사변형)\n",
    "    zoom_range=0.15,            # 무작위 줌 범위 20%\n",
    "    horizontal_flip=True,    # 무작위로 가로로 뒤짚는다.\n",
    "    vertical_flip=True\n",
    ")\n",
    "valGen = ImageDataGenerator(\n",
    "    rescale=1./255,       \n",
    "    rotation_range=30,   \n",
    "    shear_range=0.2,      \n",
    "    zoom_range=0.15,            \n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "# flow_from_directory() : 각 이미지마다 디렉토리 이름을 레이블로 사용\n",
    "train_generator = trainGen.flow_from_directory(      \n",
    "   TRAIN_PATH,\n",
    "   class_mode=\"categorical\",\n",
    "   target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "   shuffle=True,\n",
    "   batch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the validation generator\n",
    "# flow_from_directory() : 각 이미지마다 디렉토리 이름을 레이블로 사용\n",
    "validation_generator = valGen.flow_from_directory(  \n",
    "   VAL_PATH,\n",
    "   class_mode=\"categorical\",\n",
    "   target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "   shuffle=True,\n",
    "   batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117e32d-6c04-4b78-a97d-51e793d6f08c",
   "metadata": {},
   "source": [
    "### base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ca9f4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/f448gzfd2rd9vk0q5h6wlvn80000gn/T/ipykernel_2564/228766123.py:56: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  H = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[1;32m     53\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,AUC(multi_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m21\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLearningRateScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/keras/engine/training.py:2636\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m \n\u001b[1;32m   2626\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2634\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2635\u001b[0m )\n\u001b[0;32m-> 2636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:959\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    962\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    963\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    964\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensor/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers  import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.metrics import AUC\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "# ResNet50V2 모델을 불러오고, 그 모델의 출력을 활용하여 새로운 fully connected layer를 추가\n",
    "resnet = ResNet50V2(include_top=False, weights='imagenet', input_shape = (224,224,3))\n",
    "\n",
    "x = resnet.output\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x =  Dense(512, activation='relu', input_dim= (224,224,3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(21, activation='softmax')(x)  # 여기에서 Dense를 class 수에 맞게 설정해야 함.\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "\n",
    "# 기존 학습된 layer들을 freazing(새로운 layer들만 학습하도록)\n",
    "for layer in resnet.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "# define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        lr = lr * 0.1\n",
    "    return lr\n",
    "\n",
    "# compile the model with the learning rate scheduler\n",
    "opt = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\",AUC(multi_label=True,num_labels=21,name='AUC')])\n",
    "\n",
    "# train the model\n",
    "H = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
    ")\n",
    "\n",
    "#model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a585f3",
   "metadata": {},
   "source": [
    "### my_model_parameter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f7639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/f448gzfd2rd9vk0q5h6wlvn80000gn/T/ipykernel_1800/1010919424.py:56: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  H = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 100s 946ms/step - loss: 1.6903 - accuracy: 0.5270 - AUC: 0.8947 - val_loss: 6.7268 - val_accuracy: 0.1402 - val_AUC: 0.6779 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 89s 919ms/step - loss: 0.9028 - accuracy: 0.7369 - AUC: 0.9651 - val_loss: 2.0106 - val_accuracy: 0.5353 - val_AUC: 0.8871 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 89s 925ms/step - loss: 0.6789 - accuracy: 0.7959 - AUC: 0.9795 - val_loss: 1.4864 - val_accuracy: 0.6522 - val_AUC: 0.9271 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 88s 917ms/step - loss: 0.5745 - accuracy: 0.8299 - AUC: 0.9842 - val_loss: 0.8382 - val_accuracy: 0.7594 - val_AUC: 0.9696 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 88s 913ms/step - loss: 0.5113 - accuracy: 0.8453 - AUC: 0.9864 - val_loss: 0.7986 - val_accuracy: 0.7754 - val_AUC: 0.9711 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 87s 908ms/step - loss: 0.4495 - accuracy: 0.8601 - AUC: 0.9900 - val_loss: 0.9272 - val_accuracy: 0.7496 - val_AUC: 0.9609 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 87s 905ms/step - loss: 0.4154 - accuracy: 0.8753 - AUC: 0.9914 - val_loss: 2.1893 - val_accuracy: 0.5179 - val_AUC: 0.8816 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 88s 910ms/step - loss: 0.4076 - accuracy: 0.8749 - AUC: 0.9916 - val_loss: 1.4960 - val_accuracy: 0.6147 - val_AUC: 0.9243 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 87s 907ms/step - loss: 0.3678 - accuracy: 0.8820 - AUC: 0.9920 - val_loss: 1.5571 - val_accuracy: 0.6067 - val_AUC: 0.9348 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 87s 909ms/step - loss: 0.3167 - accuracy: 0.9013 - AUC: 0.9945 - val_loss: 1.0000 - val_accuracy: 0.7411 - val_AUC: 0.9596 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.2312 - accuracy: 0.9300 - AUC: 0.9964 - val_loss: 0.3189 - val_accuracy: 0.9098 - val_AUC: 0.9914 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 88s 913ms/step - loss: 0.1686 - accuracy: 0.9497 - AUC: 0.9983 - val_loss: 0.3045 - val_accuracy: 0.9121 - val_AUC: 0.9901 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 87s 908ms/step - loss: 0.1698 - accuracy: 0.9461 - AUC: 0.9983 - val_loss: 0.2682 - val_accuracy: 0.9246 - val_AUC: 0.9947 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 88s 909ms/step - loss: 0.1418 - accuracy: 0.9560 - AUC: 0.9989 - val_loss: 0.2579 - val_accuracy: 0.9254 - val_AUC: 0.9947 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 87s 908ms/step - loss: 0.1442 - accuracy: 0.9540 - AUC: 0.9988 - val_loss: 0.2598 - val_accuracy: 0.9290 - val_AUC: 0.9951 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 88s 909ms/step - loss: 0.1289 - accuracy: 0.9580 - AUC: 0.9992 - val_loss: 0.2659 - val_accuracy: 0.9250 - val_AUC: 0.9927 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 88s 914ms/step - loss: 0.1145 - accuracy: 0.9655 - AUC: 0.9991 - val_loss: 0.2391 - val_accuracy: 0.9326 - val_AUC: 0.9945 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 88s 915ms/step - loss: 0.1162 - accuracy: 0.9649 - AUC: 0.9990 - val_loss: 0.2427 - val_accuracy: 0.9326 - val_AUC: 0.9948 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.1045 - accuracy: 0.9664 - AUC: 0.9993 - val_loss: 0.2387 - val_accuracy: 0.9304 - val_AUC: 0.9940 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 5.0000002374872565e-05.\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 88s 917ms/step - loss: 0.0958 - accuracy: 0.9715 - AUC: 0.9994 - val_loss: 0.2386 - val_accuracy: 0.9348 - val_AUC: 0.9934 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 2.5000001187436284e-06.\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 88s 915ms/step - loss: 0.0957 - accuracy: 0.9721 - AUC: 0.9992 - val_loss: 0.2372 - val_accuracy: 0.9330 - val_AUC: 0.9947 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.0935 - accuracy: 0.9710 - AUC: 0.9994 - val_loss: 0.2346 - val_accuracy: 0.9321 - val_AUC: 0.9946 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 88s 912ms/step - loss: 0.1003 - accuracy: 0.9694 - AUC: 0.9993 - val_loss: 0.2311 - val_accuracy: 0.9326 - val_AUC: 0.9941 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 89s 920ms/step - loss: 0.1014 - accuracy: 0.9671 - AUC: 0.9992 - val_loss: 0.2419 - val_accuracy: 0.9308 - val_AUC: 0.9933 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 88s 915ms/step - loss: 0.0908 - accuracy: 0.9730 - AUC: 0.9993 - val_loss: 0.2524 - val_accuracy: 0.9304 - val_AUC: 0.9927 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.0935 - accuracy: 0.9710 - AUC: 0.9995 - val_loss: 0.2344 - val_accuracy: 0.9335 - val_AUC: 0.9935 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.0928 - accuracy: 0.9722 - AUC: 0.9995 - val_loss: 0.2352 - val_accuracy: 0.9353 - val_AUC: 0.9924 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 88s 911ms/step - loss: 0.0946 - accuracy: 0.9688 - AUC: 0.9996 - val_loss: 0.2305 - val_accuracy: 0.9411 - val_AUC: 0.9930 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 88s 910ms/step - loss: 0.0989 - accuracy: 0.9685 - AUC: 0.9995 - val_loss: 0.2382 - val_accuracy: 0.9348 - val_AUC: 0.9922 - lr: 2.5000e-06\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 2.5000001642183634e-06.\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 88s 916ms/step - loss: 0.0905 - accuracy: 0.9726 - AUC: 0.9995 - val_loss: 0.2235 - val_accuracy: 0.9384 - val_AUC: 0.9932 - lr: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers  import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.metrics import AUC\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "# ResNet50V2 모델을 불러오고, 그 모델의 출력을 활용하여 새로운 fully connected layer를 추가\n",
    "resnet = ResNet50V2(include_top=False, weights='imagenet', input_shape = (224,224,3))\n",
    "\n",
    "x = resnet.output\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x =  Dense(512, activation='relu', input_dim= (224,224,3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(21, activation='softmax')(x)  # 여기에서 Dense를 class 수에 맞게 설정해야 함.\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "\n",
    "# 기존 학습된 layer들을 freazing(새로운 layer들만 학습하도록)\n",
    "for layer in resnet.layers:\n",
    "   layer.trainable = True\n",
    "\n",
    "# define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        lr = lr * 0.05\n",
    "    return lr\n",
    "\n",
    "# compile the model with the learning rate scheduler\n",
    "opt = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\",AUC(multi_label=True,num_labels=21,name='AUC')])\n",
    "\n",
    "# train the model\n",
    "H = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
    ")\n",
    "\n",
    "#model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d16f3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_parameter_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87148d",
   "metadata": {},
   "source": [
    "### my_model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d6c4e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/f448gzfd2rd9vk0q5h6wlvn80000gn/T/ipykernel_1679/1207415666.py:62: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  H = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 11:14:37.990535: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 93s 695ms/step - loss: 1.9715 - accuracy: 0.4458 - AUC: 0.8606 - val_loss: 2.8627 - val_accuracy: 0.4796 - val_AUC: 0.8570 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/40\n",
      "126/126 [==============================] - 86s 682ms/step - loss: 1.1205 - accuracy: 0.6686 - AUC: 0.9505 - val_loss: 2.2909 - val_accuracy: 0.5363 - val_AUC: 0.8862 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.8445 - accuracy: 0.7413 - AUC: 0.9701 - val_loss: 1.3986 - val_accuracy: 0.6933 - val_AUC: 0.9331 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/40\n",
      "126/126 [==============================] - 86s 680ms/step - loss: 0.7305 - accuracy: 0.7725 - AUC: 0.9779 - val_loss: 1.4118 - val_accuracy: 0.6163 - val_AUC: 0.9310 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.6519 - accuracy: 0.8021 - AUC: 0.9802 - val_loss: 1.2822 - val_accuracy: 0.6467 - val_AUC: 0.9514 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.5502 - accuracy: 0.8302 - AUC: 0.9859 - val_loss: 1.7054 - val_accuracy: 0.6059 - val_AUC: 0.9221 - lr: 0.0010\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 7/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.5327 - accuracy: 0.8364 - AUC: 0.9870 - val_loss: 1.1940 - val_accuracy: 0.6867 - val_AUC: 0.9540 - lr: 0.0010\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 8/40\n",
      "126/126 [==============================] - 85s 676ms/step - loss: 0.4694 - accuracy: 0.8504 - AUC: 0.9880 - val_loss: 1.0791 - val_accuracy: 0.7015 - val_AUC: 0.9530 - lr: 0.0010\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 9/40\n",
      "126/126 [==============================] - 86s 678ms/step - loss: 0.4352 - accuracy: 0.8646 - AUC: 0.9912 - val_loss: 3.1106 - val_accuracy: 0.4019 - val_AUC: 0.8155 - lr: 0.0010\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 10/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.4298 - accuracy: 0.8653 - AUC: 0.9898 - val_loss: 1.7910 - val_accuracy: 0.5696 - val_AUC: 0.9170 - lr: 0.0010\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 11/40\n",
      "126/126 [==============================] - 86s 678ms/step - loss: 0.2841 - accuracy: 0.9114 - AUC: 0.9960 - val_loss: 0.3382 - val_accuracy: 0.8985 - val_AUC: 0.9917 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 12/40\n",
      "126/126 [==============================] - 86s 679ms/step - loss: 0.2278 - accuracy: 0.9274 - AUC: 0.9973 - val_loss: 0.2864 - val_accuracy: 0.9115 - val_AUC: 0.9946 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 13/40\n",
      "126/126 [==============================] - 86s 679ms/step - loss: 0.1903 - accuracy: 0.9425 - AUC: 0.9979 - val_loss: 0.2695 - val_accuracy: 0.9152 - val_AUC: 0.9946 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 14/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.1900 - accuracy: 0.9409 - AUC: 0.9981 - val_loss: 0.2931 - val_accuracy: 0.9156 - val_AUC: 0.9942 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 15/40\n",
      "126/126 [==============================] - 86s 678ms/step - loss: 0.1534 - accuracy: 0.9564 - AUC: 0.9987 - val_loss: 0.2774 - val_accuracy: 0.9163 - val_AUC: 0.9940 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 16/40\n",
      "126/126 [==============================] - 86s 678ms/step - loss: 0.1536 - accuracy: 0.9521 - AUC: 0.9982 - val_loss: 0.2927 - val_accuracy: 0.9052 - val_AUC: 0.9938 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 17/40\n",
      "126/126 [==============================] - 86s 678ms/step - loss: 0.1351 - accuracy: 0.9577 - AUC: 0.9988 - val_loss: 0.2649 - val_accuracy: 0.9219 - val_AUC: 0.9943 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 18/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.1253 - accuracy: 0.9606 - AUC: 0.9991 - val_loss: 0.2771 - val_accuracy: 0.9152 - val_AUC: 0.9921 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 19/40\n",
      "126/126 [==============================] - 86s 677ms/step - loss: 0.1213 - accuracy: 0.9636 - AUC: 0.9990 - val_loss: 0.2703 - val_accuracy: 0.9185 - val_AUC: 0.9936 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 20/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.1114 - accuracy: 0.9660 - AUC: 0.9991 - val_loss: 0.2617 - val_accuracy: 0.9252 - val_AUC: 0.9921 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 5.000000237487257e-06.\n",
      "Epoch 21/40\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.0933 - accuracy: 0.9702 - AUC: 0.9994 - val_loss: 0.2544 - val_accuracy: 0.9270 - val_AUC: 0.9924 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 22/40\n",
      "126/126 [==============================] - 86s 680ms/step - loss: 0.0955 - accuracy: 0.9721 - AUC: 0.9996 - val_loss: 0.2531 - val_accuracy: 0.9222 - val_AUC: 0.9935 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 23/40\n",
      "126/126 [==============================] - 86s 685ms/step - loss: 0.1038 - accuracy: 0.9689 - AUC: 0.9994 - val_loss: 0.2365 - val_accuracy: 0.9289 - val_AUC: 0.9945 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 24/40\n",
      "126/126 [==============================] - 86s 683ms/step - loss: 0.0933 - accuracy: 0.9719 - AUC: 0.9995 - val_loss: 0.2540 - val_accuracy: 0.9281 - val_AUC: 0.9932 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 25/40\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.1011 - accuracy: 0.9684 - AUC: 0.9994 - val_loss: 0.2456 - val_accuracy: 0.9241 - val_AUC: 0.9941 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 26/40\n",
      "126/126 [==============================] - 87s 693ms/step - loss: 0.0934 - accuracy: 0.9724 - AUC: 0.9992 - val_loss: 0.2590 - val_accuracy: 0.9222 - val_AUC: 0.9933 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 27/40\n",
      "126/126 [==============================] - 87s 691ms/step - loss: 0.0950 - accuracy: 0.9708 - AUC: 0.9994 - val_loss: 0.2418 - val_accuracy: 0.9289 - val_AUC: 0.9947 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 28/40\n",
      "126/126 [==============================] - 87s 686ms/step - loss: 0.0787 - accuracy: 0.9780 - AUC: 0.9997 - val_loss: 0.2556 - val_accuracy: 0.9230 - val_AUC: 0.9943 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 29/40\n",
      "126/126 [==============================] - 87s 686ms/step - loss: 0.0885 - accuracy: 0.9732 - AUC: 0.9996 - val_loss: 0.2416 - val_accuracy: 0.9300 - val_AUC: 0.9949 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 5.000000328436727e-06.\n",
      "Epoch 30/40\n",
      "126/126 [==============================] - 86s 677ms/step - loss: 0.0935 - accuracy: 0.9721 - AUC: 0.9992 - val_loss: 0.2310 - val_accuracy: 0.9296 - val_AUC: 0.9941 - lr: 5.0000e-06\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 1.2500000821091817e-06.\n",
      "Epoch 31/40\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0840 - accuracy: 0.9754 - AUC: 0.9994 - val_loss: 0.2316 - val_accuracy: 0.9274 - val_AUC: 0.9950 - lr: 1.2500e-06\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 1.2500000821091817e-06.\n",
      "Epoch 32/40\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0817 - accuracy: 0.9748 - AUC: 0.9998 - val_loss: 0.2322 - val_accuracy: 0.9270 - val_AUC: 0.9944 - lr: 1.2500e-06\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1.2500000821091817e-06.\n",
      "Epoch 33/40\n",
      "126/126 [==============================] - 86s 685ms/step - loss: 0.0848 - accuracy: 0.9738 - AUC: 0.9994 - val_loss: 0.2369 - val_accuracy: 0.9315 - val_AUC: 0.9935 - lr: 1.2500e-06\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 1.2500000821091817e-06.\n",
      "Epoch 34/40\n",
      "126/126 [==============================] - 87s 689ms/step - loss: 0.0883 - accuracy: 0.9737 - AUC: 0.9996 - val_loss: 0.2423 - val_accuracy: 0.9311 - val_AUC: 0.9939 - lr: 1.2500e-06\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 1.2500000821091817e-06.\n",
      "Epoch 35/40\n",
      "126/126 [==============================] - 87s 693ms/step - loss: 0.0859 - accuracy: 0.9724 - AUC: 0.9995 - val_loss: 0.2403 - val_accuracy: 0.9285 - val_AUC: 0.9936 - lr: 1.2500e-06\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 1.2500000821091816e-07.\n",
      "Epoch 36/40\n",
      "126/126 [==============================] - 88s 695ms/step - loss: 0.0846 - accuracy: 0.9749 - AUC: 0.9997 - val_loss: 0.2501 - val_accuracy: 0.9300 - val_AUC: 0.9931 - lr: 1.2500e-07\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1.2500001389526005e-07.\n",
      "Epoch 37/40\n",
      "126/126 [==============================] - 86s 682ms/step - loss: 0.0829 - accuracy: 0.9780 - AUC: 0.9995 - val_loss: 0.2256 - val_accuracy: 0.9296 - val_AUC: 0.9943 - lr: 1.2500e-07\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 1.2500001389526005e-07.\n",
      "Epoch 38/40\n",
      "126/126 [==============================] - 88s 694ms/step - loss: 0.0922 - accuracy: 0.9714 - AUC: 0.9994 - val_loss: 0.2321 - val_accuracy: 0.9296 - val_AUC: 0.9944 - lr: 1.2500e-07\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 1.2500001389526005e-07.\n",
      "Epoch 39/40\n",
      "126/126 [==============================] - 86s 683ms/step - loss: 0.0910 - accuracy: 0.9708 - AUC: 0.9996 - val_loss: 0.2523 - val_accuracy: 0.9248 - val_AUC: 0.9921 - lr: 1.2500e-07\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 1.2500001389526005e-07.\n",
      "Epoch 40/40\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0807 - accuracy: 0.9770 - AUC: 0.9995 - val_loss: 0.2452 - val_accuracy: 0.9278 - val_AUC: 0.9936 - lr: 1.2500e-07\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers  import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from keras.metrics import AUC\n",
    "\n",
    "from keras.applications.resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2, preprocess_input, decode_predictions\n",
    "from keras.applications.resnet import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "# ResNet50V2 모델을 불러오고, 그 모델의 출력을 활용하여 새로운 fully connected layer를 추가\n",
    "resnet = ResNet50V2(include_top=False, weights='imagenet', input_shape = (224,224,3))\n",
    "\n",
    "x = resnet.output\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x =  Dense(512, activation='relu', input_dim= (224,224,3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x =  Dense(21, activation='softmax')(x)  # 여기에서 Dense를 class 수에 맞게 설정해야 함.\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "\n",
    "# 기존 학습된 layer들을 freazing(새로운 layer들만 학습하도록)\n",
    "for layer in resnet.layers:\n",
    "   layer.trainable = True\n",
    "\n",
    "# define the learning rate scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if (epoch > 0) and (epoch <= 10) and (epoch % 10 == 0):\n",
    "        lr = lr * 0.1\n",
    "    elif (epoch > 10) and (epoch <= 20) and (epoch % 10 == 0):\n",
    "        lr = lr * 0.05\n",
    "    elif (epoch > 20) and (epoch <= 30) and (epoch % 10 == 0):\n",
    "        lr = lr * 0.25\n",
    "    elif (epoch > 30) and (epoch <= 35) and (epoch % 5 == 0):\n",
    "        lr = lr * 0.1\n",
    "    return lr\n",
    "\n",
    "# compile the model with the learning rate scheduler\n",
    "opt = Adam()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\",AUC(multi_label=True,num_labels=21,name='AUC')])\n",
    "\n",
    "# train the model\n",
    "H = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=N_EPOCH,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
    ")\n",
    "\n",
    "#model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d749777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_lr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e89658",
   "metadata": {},
   "source": [
    "## 이미지 분류\n",
    "- image_predict 파일 따로 올려놓음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31938370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Found 6315 images belonging to 21 classes.\n",
      "이미지를 넣어주세요: /Users/nayoungmin/Desktop/이미지예상/IMG_5959.JPG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-17 13:00:32.720914: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 662ms/step\n",
      "워터파크: 97.99%\n",
      "아쿠아리움: 0.82%\n",
      "놀이공원: 0.31%\n",
      "철로: 0.22%\n",
      "피크닉: 0.15%\n",
      "도심하천: 0.12%\n",
      "케이블카: 0.08%\n",
      "돌담길: 0.06%\n",
      "극장: 0.05%\n",
      "전통문화: 0.05%\n",
      "벚꽃: 0.04%\n",
      "궁: 0.03%\n",
      "자연: 0.02%\n",
      "노을: 0.01%\n",
      "단풍: 0.01%\n",
      "번화가: 0.01%\n",
      "아이스링크: 0.01%\n",
      "유람선: 0.01%\n",
      "빌딩: 0.0%\n",
      "야경: 0.0%\n",
      "타워: 0.0%\n",
      "카테고리:  워터파크\n",
      "카테고리: 워터파크\n",
      "   category         name sector\n",
      "30     워터파크  SeaLaLa워터파크   워터파크\n",
      "31     워터파크         워터킹덤   워터파크\n",
      "32     워터파크   난지한공공원물놀이장    수영장\n",
      "이미지를 넣어주세요: 종료\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# 추천 여행지 파일\n",
    "recommend_csv = pd.read_csv('/Users/nayoungmin/Desktop/Capstone/recommend.csv')\n",
    "\n",
    "\n",
    "# 저장한 모델 파일 경로\n",
    "model_path = '/Users/nayoungmin/Desktop/Capstone/Code/my_model_parameter_1.h5'\n",
    "TRAIN_PATH = '/Users/nayoungmin/Desktop/Capstone/splited_image_is/train'\n",
    "\n",
    "# 저장한 모델 불러오기\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 이미지 클래스 이름 지정해주기\n",
    "classGen = ImageDataGenerator()\n",
    "\n",
    "class_generator = classGen.flow_from_directory(\n",
    "    directory=TRAIN_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "while(True):\n",
    "    img_path = input('이미지를 넣어주세요: ') # 예측하고자 하는 이미지 경로\n",
    "    if img_path == '종료':\n",
    "        break\n",
    "    target_size = (224, 224) # 모델이 학습시킨 이미지 사이즈와 동일하게 설정\n",
    "\n",
    "    # 이미지 전처리\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0 # 이미지 스케일링\n",
    "\n",
    "    # 예측\n",
    "    preds = model.predict(x)\n",
    "\n",
    "    # 클래스 이름 받는 Dictionary 만들기\n",
    "    pre_dict = {}\n",
    "\n",
    "    # 클래스별 확률 값 출력\n",
    "    class_indices = class_generator.class_indices\n",
    "    class_names = list(class_indices.keys())\n",
    "    \n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        pre_dict[class_names[i]] = round(preds[0][i]*100, 2)        \n",
    "    \n",
    "    sorted_pre_dict = sorted(pre_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for key, value in sorted_pre_dict:\n",
    "        print(f\"{key}: {value}%\")\n",
    "\n",
    "    if max(pre_dict.values()) < 40:\n",
    "        print('\\n\\n다른 사진을 업로드 해주세요!\\n\\n')\n",
    "        continue\n",
    "    else:\n",
    "        max_pre = max(pre_dict, key=pre_dict.get)\n",
    "        print('카테고리: ',max_pre)\n",
    "        \n",
    "        \n",
    "    input_rec = input('카테고리: ')\n",
    "    print(recommend_csv.loc[recommend_csv['category']==input_rec, ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
